{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b063837e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/u3/w3pang/HAO_Graphllm/llm/GRAPHOMNI_code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GRAPHOMNI Guidelines\n",
      "We provide detailed instructions for evaluation. \n",
      "To execute our evaluation script, please ensure that the structure of your model outputs is the same as ours.\n",
      "\n",
      "We provide two options:\n",
      "1. Evaluation only: you can parse the response on your own and simply provide one file with all the final predictions.\n",
      "2. Parse and evaluation: you can leave all the responses to us with the output formats shown below.\n",
      "\n",
      "# ğŸ§  Graph Reasoning Benchmark (GRB)\n",
      "\n",
      "A comprehensive framework for evaluating graph algorithm reasoning capabilities across large language models (LLMs). This project benchmarks models on fundamental graph tasks using a diverse set of prompts, graph formats, and difficulty levels.\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸš€ Overview\n",
      "\n",
      "This repository introduces a benchmarking method that evaluates LLMs' performance on classic graph problems such as **connectivity**, **shortest path**, **diameter**, and more. The framework supports multiple prompting styles, graph input formats, and difficulty modes to rigorously test model generalization and reasoning capabilities.\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸ” Supported Graph Tasks\n",
      "\n",
      "| Task Name       | Description                              |\n",
      "|-----------------|------------------------------------------|\n",
      "| `connectivity`  | Determine if a graph is connected        |\n",
      "| `bfsorder`      | Return the BFS traversal order           |\n",
      "| `triangle`      | Count or detect triangle cycles          |\n",
      "| `diameter`      | Compute the graphâ€™s diameter             |\n",
      "| `cycle`         | Detect if a cycle exists                 |\n",
      "| `shortest_path` | Find shortest paths between nodes        |\n",
      "| `all`           | Evaluate all of the above                |\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸ¤– Supported Models\n",
      "\n",
      "- `Llama3.1`\n",
      "- `Mistral`\n",
      "- `Phi-4`\n",
      "- `Qwen2.5`\n",
      "- `Qwen3-8B`\n",
      "\n",
      "---\n",
      "\n",
      "## âš™ï¸ Prompting Modes\n",
      "\n",
      "| Prompt Type    | Description                                  |\n",
      "|----------------|----------------------------------------------|\n",
      "| `Algorithm`    | Ask models to follow formal algorithms       |\n",
      "| `CoT`          | Chain-of-Thought reasoning                   |\n",
      "| `k-shot`       | k examples before test input                 |\n",
      "| `Instruct`     | Instruction-style prompting                  |\n",
      "| `none`         | No prompt guidance                           |\n",
      "| `0-CoT`        | Zero-shot Chain-of-Thought                   |\n",
      "| `0-Instruct`   | Zero-shot Instruction                        |\n",
      "| `0-Algorithm`  | Zero-shot Algorithm prompting                |\n",
      "| `LTM`          | With long-term memory context (if supported) |\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸ§© Graph Input Formats\n",
      "\n",
      "- `Graph Modelling Language (GML)`\n",
      "- `Adjacency Set`\n",
      "- `Edge Set`\n",
      "- `Edge List`\n",
      "- `Adjacency Matrix`\n",
      "- `Adjacency List`\n",
      "- `GraphML`\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸšï¸ Difficulty Modes\n",
      "\n",
      "- `easy`: Small graphs, low noise\n",
      "- `medium`: Moderate complexity\n",
      "- `hard`: Larger graphs, ambiguous structure\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸ“ Directory Structure\n",
      "\n",
      "# Evaluation Guidelines\n",
      "We provide detailed instructions for evaluation. \n",
      "To execute our evaluation script, please ensure that the structure of your model outputs is the same as ours.\n",
      "\n",
      "We provide two options:\n",
      "1. Evaluation only: you can parse the response on your own and simply provide one file with all the final predictions.\n",
      "2. Parse and evaluation: you can leave all the responses to us with the output formats shown below.\n",
      "\n",
      "## Evaluation Only\n",
      "If you want to use your own parsing logic and *only provide the final answer*, you can use `main_eval_only.py`.\n",
      "\n",
      "You can provide all the outputs in *one file* in the following format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"validation_Accounting_1\": \"D\", # strictly \"A\", \"B\", \"C\", \"D\" for multi-choice question\n",
      "    \"validation_Architecture_and_Engineering_14\": \"0.0\", # any string response for open question.\n",
      "    ...\n",
      "}\n",
      "```\n",
      "Then run eval_only with:\n",
      "```\n",
      "python main_eval_only.py --output_path ./example_outputs/llava1.5_13b/total_val_output.json\n",
      "```\n",
      "\n",
      "Please refer to [example output](https://github.com/MMMU-Benchmark/MMMU/blob/main/mmmu/example_outputs/llava1.5_13b/total_val_output.json) for a detailed prediction file form.\n",
      "\n",
      "\n",
      "## Parse and Evaluation\n",
      "You can also provide response and run the `main_parse_and_eval.py` to use our answer parsing processing and evaluation pipeline as follows:\n",
      "\n",
      "### Output folder structure\n",
      "\n",
      "```\n",
      "â””â”€â”€ model_name\n",
      "    â”œâ”€â”€ category_name (e.g., Accounting)\n",
      "    â”‚   â”œâ”€â”€ output.json\n",
      "    â””â”€â”€ category_name (e.g., Electronics)\n",
      "        â”œâ”€â”€ output.json\n",
      "    ...\n",
      "```\n",
      "\n",
      "### Output file\n",
      "Each `output.json`` has a list of dict containing instances for evaluation ().\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"id\": \"validation_Electronics_28\",\n",
      "        \"question_type\": \"multiple-choice\",\n",
      "        \"answer\": \"A\", # given answer\n",
      "        \"all_choices\": [ # create using `get_multi_choice_info` in \n",
      "            \"A\",\n",
      "            \"B\",\n",
      "            \"C\",\n",
      "            \"D\"\n",
      "        ],\n",
      "        \"index2ans\": { # create using `get_multi_choice_info` in \n",
      "            \"A\": \"75 + 13.3 cos(250t - 57.7Â°)V\",\n",
      "            \"B\": \"75 + 23.3 cos(250t - 57.7Â°)V\",\n",
      "            \"C\": \"45 + 3.3 cos(250t - 57.7Â°)V\",\n",
      "            \"D\": \"95 + 13.3 cos(250t - 57.7Â°)V\"\n",
      "        },\n",
      "        \"response\": \"B\" # model response\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"validation_Electronics_29\",\n",
      "        \"question_type\": \"short-answer\",\n",
      "        \"answer\": \"30\", # given answer\n",
      "        \"response\": \"36 watts\" # model response\n",
      "    },\n",
      "    ...\n",
      "]\n",
      "```\n",
      "\n",
      "### Evaluation\n",
      "```\n",
      "python main_parse_and_eval.py --path ./example_outputs/llava1.5_13b --subject ALL # all subject\n",
      "\n",
      "# OR you can specify one subject for the evaluation\n",
      "\n",
      "python main_parse_and_eval.py --path ./example_outputs/llava1.5_13b --subject elec # short name for Electronics. use --help for all short names\n",
      "\n",
      "```\n",
      "\n",
      "`main_parse_and_eval.py` will generate `parsed_output.json` and `result.json` in the subfolder under the same category with output.json, respectively.\n",
      "\n",
      "```\n",
      "â”œâ”€â”€ Accounting\n",
      "â”‚   â”œâ”€â”€ output.json\n",
      "â”‚   â”œâ”€â”€ parsed_output.json\n",
      "â”‚   â””â”€â”€ result.json\n",
      "â””â”€â”€ Electronics\n",
      "    â”œâ”€â”€ output.json\n",
      "    â”œâ”€â”€ parsed_output.json\n",
      "    â””â”€â”€ result.json\n",
      "...\n",
      "```\n",
      "\n",
      "### Print Results\n",
      "You can print results locally if you want. (use `pip install tabulate` if you haven't)\n",
      "```\n",
      "python print_results.py --path ./example_outputs/llava1.5_13b\n",
      "# Results may be slightly different due to the ramdon selection for fail response\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "##### Run Llava\n",
      "In case if you want to reproduce the results of some models, please go check `run_llava.py` as an example.\n",
      "\n",
      "By setting up the env for llava via following steps:\n",
      "\n",
      "Step 1:\n",
      "```\n",
      "git clone https://github.com/haotian-liu/LLaVA.git\n",
      "cd LLaVA\n",
      "```\n",
      "In Step 2:\n",
      "```\n",
      "conda create -n llava python=3.10 -y\n",
      "conda activate llava\n",
      "pip install --upgrade pip  # enable PEP 660 support\n",
      "git fetch --tags  \n",
      "git checkout tags/v1.1.3  # back to the version when running MMMU\n",
      "pip install -e .\n",
      "```\n",
      "\n",
      "The above will install llava (1.5 only) and corresponding correct transformers version when running MMMU.\n",
      "Then by installing `datasets` packages from huggingface (i.e., `pip install datasets`), you can run llava with the following command:\n",
      "\n",
      "```\n",
      "CUDA_VISIBLE_DEVICES=0 nohup python run_llava.py \\\n",
      "--output_path example_outputs/llava1.5_13b_val.json \\\n",
      "--model_path liuhaotian/llava-v1.5-13b \\\n",
      "--config_path configs/llava1.5.yaml\n",
      "```\n",
      "\n",
      "Then you can evaluate the results via the very first pipeline.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨å½“å‰å•å…ƒæ ¼æˆ–ä¸Šä¸€ä¸ªå•å…ƒæ ¼ä¸­æ‰§è¡Œä»£ç æ—¶ Kernel å´©æºƒã€‚\n",
      "\u001b[1;31mè¯·æŸ¥çœ‹å•å…ƒæ ¼ä¸­çš„ä»£ç ï¼Œä»¥ç¡®å®šæ•…éšœçš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må•å‡»<a href='https://aka.ms/vscodeJupyterKernelCrash'>æ­¤å¤„</a>äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚\n",
      "\u001b[1;31mæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ Jupyter <a href='command:jupyter.viewOutput'>log</a>ã€‚"
     ]
    }
   ],
   "source": [
    "with open('README.md', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    print(content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
